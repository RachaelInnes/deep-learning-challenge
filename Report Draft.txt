NOTES FOR ASSIGNEMENT 

Overview and Objective:
The purpose of this project was to develop a machine learning model for Alaphabet Soup (a nonprofit foundation) that can predict 
if applicant will be succesful if funded by Alphabet Soup.Such analysis assist the foundation in selecting the best applicant for funding. 
This information is important for Alphabet Soup to help with their decisions in regards to who should and should not receive fnding from the foundation. 

Dataset Overview:
The data provided was in a CSV format and contained information from *** organisations.

Results:

Data Preprocessing

The target variable for this model is "IS_SUCCESSFUL", this is the binary variable that indicates if funding was used effectively by the applicant (the organisation).

The features for your model
EIN and NAME—Identification columns
APPLICATION_TYPE—Alphabet Soup application type
AFFILIATION—Affiliated sector of industry
CLASSIFICATION—Government organisation classification
USE_CASE—Use case for funding
ORGANIZATION—Organisation type
STATUS—Active status
INCOME_AMT—Income classification
SPECIAL_CONSIDERATIONS—Special considerations for application
ASK_AMT—Funding amount requested
IS_SUCCESSFUL—Was the money used effectively

The EIN and NAME identification columns were removed from the data set. These variables were removed as they did not provide any insights to the data, each EIN and Name awas attacehd to an Application Type and other Classifications
they were not needed to identify the organisations or used for analysis. 

Compiling, Training and Evaluating the Model 
I conducted four training models in an attempt to reach 75% accuracy in predicting success of funding, in all models I adopted the following: 

-Activation function: The ReLU activation function is used in the hidden layers to introduce non-linearity, and the sigmoid activation function in the output layer is used to predict the probability of success.
-Output layer: The outer layer utilised the Sigmoid activatio function with a single (1) neuron.
-Neurons: In all models I reduced the number of neurons with each hidden layer.
-THe model saved weights every 5 eopshc, this is to monitor the training progrress. 

Model 1:Starter Code
Model used: for the original model I built a deep neural network with two hidden layers and an output layer designed for binary classification. 
First hidden layer: The first hidden layer had 50 neurons to align to the number of input features (49)
Second layer: The second layer had 25 neurons.
Epochs:100
Batch: 320

Evaluation of the model:
The accuracy of the model was:
215/215 - 0s - 2ms/step - accuracy: 0.7261 - loss: 0.5974
Loss: 0.5974100232124329, Accuracy: 0.7260932922363281

Overall this model had 72.61% accuracy with a loss of 59.00% 

This model correctly predicted the outcome of funding being used successfully or not 72.61 % of the time. 59% loss suggest that the model is still learning and may need some adjustments to improve the accuracy.

************************************

Model 2:Starter Code 1
Model used: for the original model I built a deep neural network with two hidden layers and an output layer designed for binary classification. 
First hidden layer: The first hidden layer had 60 neurons.
Second layer: The second layer had 35 neurons.
Third layer: The third layer had 15 neurons. 
 layer is used to predict the probability of success.

In this model I added a third layer, the rationale for adding an additional layer is to ehance the model to hopefully learn more complex patterns of the data. The first model provided a good baseline,
however, the aim is to to reach 75% accuracy, hence the addition of a layer and changes to the neurons. 

Evaluation of the model:
The accuracy of the model was:
215/215 - 1s - 3ms/step - accuracy: 0.7284 - loss: 0.5595
Loss: 0.5595238208770752, Accuracy: 0.7284256815910339

Overall this model had 72.84% accuracy with a loss of 55.95% 

****************************************

Model 3: Starter Code 2
In this model I added an additional layer in an attempt to enhance the ability to read more complex data patterns and enhance the accuracy. 
First hidden layer: The first hidden layer had 40 neurons.
Second layer: The second layer had 30 neurons.
Third layer: The third layer had 20neurons. 
Fourth layer: The fourth layer had 10 neurons
Changed the Epochs: Epoch - 150, Batch - 128


Evaluation of this model:
215/215 - 0s - 2ms/step - accuracy: 0.7278 - loss: 0.5647
Loss: 0.5646637678146362, Accuracy: 0.7278425693511963

Overall this model had 72.78% accuracy with a loss of 56.47% 

*******************************************************

Model 4: Starter Code 3
In this model I went back to a third layer:
First hidden layer: The first hidden layer had 256 neurons.
Second layer: The second layer had 128 neurons.
Third layer: The third layer had 64 neurons. 

I have tried batch normalization and dropout to reduce the overfitting, which may account for the accuracy not improving in previous model. I have also implemented 'EarlyStopping", upon investigation this 
was suggested once again to manage any overfitting, this stops the training when the validation loss stops imprveing. I also implemented the Reduce LROnPlateu which adjusts the learning rate of the model, this assists
in fine tuning the process to assist the learning rate being too high (and thus missing optimal solutions) or too low (and thus, slow).


Evaluation of this model:
215/215 - 0s - 1ms/step - accuracy: 0.7305 - loss: 0.5516
Loss: 0.5515720248222351, Accuracy: 0.7304664850234985

Overall this model had 73.04% accuracy with a loss of 55.16% 


Summary and Recommendations: I developed four models to improve the accuracy, the highest level of accuracy I achieved was 73.04%. In terms of Alphabet Soup, although this may be considered ok it does
present some risk, in that, 27% of cases they may not be providing funding to the best applicants. There could be other strategies implemented to further improve the training model. 


